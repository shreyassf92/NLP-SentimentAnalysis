{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UcIupPbGgZZq"
   },
   "source": [
    "# NLP text sentiment analysis using tenorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NHXi7Xv2gZZs"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-66fde16f9f85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VdwkYiWWgZZv"
   },
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 100\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "training_size = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J6877nnrgZZz"
   },
   "outputs": [],
   "source": [
    "#load preprocessed data\n",
    "data = pd.read_csv(\"/content/data_WithOutProductTitleWords_WithoutLessFrequentWords.tsv\", sep='\\t')\n",
    "\n",
    "data.dropna(axis='rows', inplace=True)\n",
    "\n",
    "def convertTextToList(x):\n",
    "    text = list(x.split(\" \")) \n",
    "    return text\n",
    "\n",
    "data['review_body_cleaned'].apply(lambda x: convertTextToList(x))\n",
    "\n",
    "#split data set before vectorization\n",
    "train, test = train_test_split(data, test_size = 0.2, stratify = data['class'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "44d6idsKgZZ4",
    "outputId": "c2055bad-5302-477c-c2ca-85e650b96982"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3703\n"
     ]
    }
   ],
   "source": [
    "#Tokenize\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train['review_body_cleaned'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(len(word_index))\n",
    "\n",
    "#train words\n",
    "training_sequences = tokenizer.texts_to_sequences(train['review_body_cleaned'])\n",
    "#training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type)\n",
    "\n",
    "#test words\n",
    "testing_sequences = tokenizer.texts_to_sequences(test['review_body_cleaned'])\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "phHTOJiggZZ7"
   },
   "outputs": [],
   "source": [
    "#Model Simple NN\n",
    "model_nn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 25, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_nn.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "colab_type": "code",
    "id": "PTtXN4OjgZZ-",
    "outputId": "68d6f2be-4d04-43f3-c601-0d7cea9db7e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 25)           92600     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 24)                624       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 93,249\n",
      "Trainable params: 93,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "2547/2547 - 7s - loss: 0.4136 - accuracy: 0.8166 - val_loss: 0.3314 - val_accuracy: 0.8527\n",
      "Epoch 2/10\n",
      "2547/2547 - 7s - loss: 0.3148 - accuracy: 0.8650 - val_loss: 0.3210 - val_accuracy: 0.8607\n",
      "Epoch 3/10\n",
      "2547/2547 - 7s - loss: 0.3025 - accuracy: 0.8730 - val_loss: 0.3217 - val_accuracy: 0.8630\n",
      "Epoch 4/10\n",
      "2547/2547 - 7s - loss: 0.2944 - accuracy: 0.8757 - val_loss: 0.3160 - val_accuracy: 0.8637\n",
      "Epoch 5/10\n",
      "2547/2547 - 7s - loss: 0.2893 - accuracy: 0.8773 - val_loss: 0.3187 - val_accuracy: 0.8625\n",
      "Epoch 6/10\n",
      "2547/2547 - 7s - loss: 0.2849 - accuracy: 0.8787 - val_loss: 0.3179 - val_accuracy: 0.8616\n",
      "Epoch 7/10\n",
      "2547/2547 - 7s - loss: 0.2817 - accuracy: 0.8793 - val_loss: 0.3173 - val_accuracy: 0.8651\n",
      "Epoch 8/10\n",
      "2547/2547 - 7s - loss: 0.2784 - accuracy: 0.8806 - val_loss: 0.3176 - val_accuracy: 0.8657\n",
      "Epoch 9/10\n",
      "2547/2547 - 7s - loss: 0.2755 - accuracy: 0.8817 - val_loss: 0.3189 - val_accuracy: 0.8636\n",
      "Epoch 10/10\n",
      "2547/2547 - 7s - loss: 0.2719 - accuracy: 0.8836 - val_loss: 0.3211 - val_accuracy: 0.8630\n"
     ]
    }
   ],
   "source": [
    "print(model_nn.summary())\n",
    "\n",
    "num_epochs = 10\n",
    "history = model_nn.fit(training_padded, train['class'], epochs=num_epochs, validation_data=(testing_padded,  test['class']), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hrSvFo71gZaB",
    "outputId": "6748c94c-39cb-4a9f-9516-cc907d778d07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7143575]]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"like it\"]\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "padded = pad_sequences(sequences, maxlen=100, padding=padding_type )\n",
    "print(model_nn.predict(padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "swJQW8YjgZaE"
   },
   "outputs": [],
   "source": [
    "#Model RNN \n",
    "\n",
    "vocab_size = len(word_index)+1\n",
    "max_length = 100\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 100, input_length=max_length),\n",
    "    tf.keras.layers.GRU(units=125),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "#model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tYlxiVOygZaH"
   },
   "outputs": [],
   "source": [
    "print(model.summary())\n",
    "\n",
    "num_epochs = 10\n",
    "history = model.fit(training_padded, train['class'], epochs=num_epochs, validation_data=(testing_padded,  test['class']), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qw6ARBVKgZaK"
   },
   "outputs": [],
   "source": [
    "sentence = [\"doesn't fit the product is not good at all\"]\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "padded = pad_sequences(sequences, maxlen=100, padding=padding_type )\n",
    "print(model.predict(padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T-eG5-C2gZaN"
   },
   "outputs": [],
   "source": [
    "#Model RNN with GloVe embedding\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "vocab_size = len(word_index)+1\n",
    "\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.100d.txt',  encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > vocab_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Iwt7_aOgZaQ"
   },
   "outputs": [],
   "source": [
    "#Model RNN - GRU \n",
    "\n",
    "vocab_size = len(word_index)+1\n",
    "max_length = 100\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 100, input_length=max_length, weights=[embedding_matrix], trainable=True),\n",
    "    tf.keras.layers.GRU(units=250),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "#model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "colab_type": "code",
    "id": "79GFnv0LgZaV",
    "outputId": "e715d9b2-f529-415f-ba46-fcbbeef7a018"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          370400    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 250)               264000    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 634,651\n",
      "Trainable params: 634,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "2547/2547 - 689s - loss: 0.5467 - accuracy: 0.7654 - val_loss: 0.5476 - val_accuracy: 0.7657\n",
      "Epoch 2/15\n",
      "2547/2547 - 682s - loss: 0.5107 - accuracy: 0.7785 - val_loss: 0.3235 - val_accuracy: 0.8562\n",
      "Epoch 3/15\n",
      "2547/2547 - 683s - loss: 0.3031 - accuracy: 0.8672 - val_loss: 0.2997 - val_accuracy: 0.8680\n",
      "Epoch 4/15\n",
      "2547/2547 - 686s - loss: 0.2755 - accuracy: 0.8818 - val_loss: 0.3005 - val_accuracy: 0.8664\n",
      "Epoch 5/15\n",
      "2547/2547 - 692s - loss: 0.2521 - accuracy: 0.8922 - val_loss: 0.3171 - val_accuracy: 0.8645\n",
      "Epoch 6/15\n",
      "2547/2547 - 694s - loss: 0.2234 - accuracy: 0.9057 - val_loss: 0.3296 - val_accuracy: 0.8597\n",
      "Epoch 7/15\n",
      "2547/2547 - 686s - loss: 0.1923 - accuracy: 0.9196 - val_loss: 0.3808 - val_accuracy: 0.8561\n",
      "Epoch 8/15\n",
      "2547/2547 - 687s - loss: 0.1646 - accuracy: 0.9321 - val_loss: 0.4385 - val_accuracy: 0.8502\n",
      "Epoch 9/15\n",
      "2547/2547 - 692s - loss: 0.1418 - accuracy: 0.9420 - val_loss: 0.4845 - val_accuracy: 0.8484\n",
      "Epoch 10/15\n",
      "2547/2547 - 694s - loss: 0.1265 - accuracy: 0.9494 - val_loss: 0.5440 - val_accuracy: 0.8466\n",
      "Epoch 11/15\n",
      "2547/2547 - 708s - loss: 0.1121 - accuracy: 0.9548 - val_loss: 0.6134 - val_accuracy: 0.8501\n",
      "Epoch 12/15\n",
      "2547/2547 - 693s - loss: 0.1034 - accuracy: 0.9583 - val_loss: 0.6288 - val_accuracy: 0.8438\n",
      "Epoch 13/15\n",
      "2547/2547 - 692s - loss: 0.0952 - accuracy: 0.9610 - val_loss: 0.6165 - val_accuracy: 0.8438\n",
      "Epoch 14/15\n",
      "2547/2547 - 697s - loss: 0.0896 - accuracy: 0.9640 - val_loss: 0.6912 - val_accuracy: 0.8474\n",
      "Epoch 15/15\n",
      "2547/2547 - 701s - loss: 0.0843 - accuracy: 0.9659 - val_loss: 0.7032 - val_accuracy: 0.8470\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())\n",
    "\n",
    "num_epochs = 15\n",
    "history = model.fit(training_padded, train['class'], epochs=num_epochs, validation_data=(testing_padded,  test['class']), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eQ6hlmlvgZaX",
    "outputId": "0b834925-460d-4745-a48f-cb7f9b9bfd33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33747125]]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"dont like it\"]\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "padded = pad_sequences(sequences, maxlen=100, padding=padding_type )\n",
    "print(model.predict(padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fwXJb8kRgZaZ"
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"GRU_vocab_size_100_maxlength_TrainableTrue_250_Sigmoid_epoch15.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"GRU_vocab_size_100_maxlength_TrainableTrue_250_Sigmoid_epoch15.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ys-3ShTf-dX6"
   },
   "outputs": [],
   "source": [
    "#Model RNN - LSTM \n",
    "\n",
    "vocab_size = len(word_index)+1\n",
    "max_length = 100\n",
    "\n",
    "model_lstm = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 100, input_length=max_length, weights=[embedding_matrix], trainable=True),\n",
    "    tf.keras.layers.LSTM(units=250),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_lstm.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "#model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "colab_type": "code",
    "id": "AU7G_nvq-ln7",
    "outputId": "aa69b28d-1553-4e95-9191-0c25ec3f8ded"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          370400    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 250)               351000    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 721,651\n",
      "Trainable params: 721,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "2547/2547 - 833s - loss: 0.5467 - accuracy: 0.7655 - val_loss: 0.5466 - val_accuracy: 0.7657\n",
      "Epoch 2/15\n",
      "2547/2547 - 839s - loss: 0.5450 - accuracy: 0.7657 - val_loss: 0.5442 - val_accuracy: 0.7657\n",
      "Epoch 3/15\n",
      "2547/2547 - 839s - loss: 0.5448 - accuracy: 0.7657 - val_loss: 0.5442 - val_accuracy: 0.7657\n",
      "Epoch 4/15\n",
      "2547/2547 - 834s - loss: 0.5444 - accuracy: 0.7658 - val_loss: 0.5443 - val_accuracy: 0.7656\n",
      "Epoch 5/15\n",
      "2547/2547 - 836s - loss: 0.5440 - accuracy: 0.7659 - val_loss: 0.5448 - val_accuracy: 0.7657\n",
      "Epoch 6/15\n",
      "2547/2547 - 846s - loss: 0.5439 - accuracy: 0.7660 - val_loss: 0.5455 - val_accuracy: 0.7652\n",
      "Epoch 7/15\n",
      "2547/2547 - 844s - loss: 0.5437 - accuracy: 0.7660 - val_loss: 0.5454 - val_accuracy: 0.7649\n",
      "Epoch 8/15\n",
      "2547/2547 - 840s - loss: 0.5437 - accuracy: 0.7660 - val_loss: 0.5453 - val_accuracy: 0.7651\n",
      "Epoch 9/15\n",
      "2547/2547 - 840s - loss: 0.4551 - accuracy: 0.8010 - val_loss: 0.3253 - val_accuracy: 0.8585\n",
      "Epoch 10/15\n",
      "2547/2547 - 845s - loss: 0.3011 - accuracy: 0.8686 - val_loss: 0.3070 - val_accuracy: 0.8665\n",
      "Epoch 11/15\n",
      "2547/2547 - 849s - loss: 0.2779 - accuracy: 0.8794 - val_loss: 0.3059 - val_accuracy: 0.8694\n",
      "Epoch 12/15\n",
      "2547/2547 - 843s - loss: 0.2599 - accuracy: 0.8867 - val_loss: 0.3202 - val_accuracy: 0.8662\n",
      "Epoch 13/15\n",
      "2547/2547 - 842s - loss: 0.2394 - accuracy: 0.8954 - val_loss: 0.3301 - val_accuracy: 0.8655\n",
      "Epoch 14/15\n",
      "2547/2547 - 836s - loss: 0.2165 - accuracy: 0.9060 - val_loss: 0.3438 - val_accuracy: 0.8612\n",
      "Epoch 15/15\n",
      "2547/2547 - 854s - loss: 0.1928 - accuracy: 0.9181 - val_loss: 0.3663 - val_accuracy: 0.8556\n"
     ]
    }
   ],
   "source": [
    "print(model_lstm.summary())\n",
    "\n",
    "num_epochs = 15\n",
    "history = model_lstm.fit(training_padded, train['class'], epochs=num_epochs, validation_data=(testing_padded,  test['class']), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c1lTS9Re2lF-",
    "outputId": "7716abd7-f821-436a-f4c2-7da10018d81d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17820473]]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"\"]\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "padded = pad_sequences(sequences, maxlen=100, padding=padding_type )\n",
    "print(model_lstm.predict(padded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ISJSc5GiQTw"
   },
   "outputs": [],
   "source": [
    "model_json = model_lstm.to_json()\n",
    "with open(\"LSTM_vocab_size_100_maxlength_TrainableTrue_250_Sigmoid_epoch15.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model_lstm.save_weights(\"LSTM_vocab_size_100_maxlength_TrainableTrue_250_Sigmoid_epoch15.h5\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_NN_TF.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
