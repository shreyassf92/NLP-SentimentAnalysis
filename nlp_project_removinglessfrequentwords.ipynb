{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_project_removinglessfrequentwords.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JslLi0xNfl8-",
        "outputId": "5da3a758-c383-416b-e45e-46167ddf40a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#initialization\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"words\")\n",
        "nltk.download('wordnet')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "englishwords = nltk.corpus.words.words()\n",
        "\n",
        "spacy.load('en_core_web_sm')\n",
        "\n",
        "spacy_nlp = spacy.load('en_core_web_sm')\n",
        "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "for word in spacy_stopwords:\n",
        "  if(word not in stopwords):\n",
        "    stopwords.append(word)\n",
        "\n",
        "#import data\n",
        "#data = pd.read_csv(\"amazon_reviews_us_Mobile_Electronics_v1.tsv\", sep='\\t')\n",
        "\n",
        "#data = data[['product_title', 'product_category', 'review_headline', 'review_body', 'star_rating']]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCw3JfUngD84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%negation word list\n",
        "negationwords = [\"no\", \"not\", \"none\", \"no one\", \"nobody\", \"nothing\", \"neither\", \"nowhere\", \"never\", \"hardly\", \"scarcely\", \"barely\", \"doesnt\", \"isnt\", \"wasnt\", \"shouldnt\", \"wouldnt\", \"couldnt\", \"wont\", \"cant\", \"dont\", \"didnt\"]\n",
        "new_stopwords = []\n",
        "\n",
        "#create unimportant words from the corpus\n",
        "unimportant_words = []\n",
        "data['product_title'].apply(lambda x: create_unimportantwords(x))\n",
        "print(len(unimportant_words))\n",
        "\n",
        "#load lexicon words\n",
        "lexiconwords = []\n",
        "\n",
        "with open('/content/lexiconwords.txt', 'r') as filehandle:\n",
        "    filecontents = filehandle.readlines()\n",
        "    for line in filecontents:\n",
        "        current_place = line[:-1]\n",
        "        lexiconwords.append(current_place)\n",
        "\n",
        "unimportant_words = [word for word in unimportant_words if word not in lexiconwords]\n",
        "print(len(unimportant_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_syNyZGgftAO",
        "colab": {}
      },
      "source": [
        "#%% functions\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    cleaned_text = \"\".join([char for char in text if char not in string.punctuation])\n",
        "    return cleaned_text\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = re.split('\\W+', text)\n",
        "    return tokens\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    new_stopwords = [word for word in stopwords if word not in negationwords]\n",
        "    new_stopwords.extend(unimportant_words)\n",
        "    required_tokens = [word for word in tokens if word not in new_stopwords]\n",
        "    return required_tokens\n",
        "\n",
        "def keeponly_alpha(tokens):\n",
        "    aplha_tokens = [word for word in tokens if word.isalpha()]\n",
        "    return aplha_tokens\n",
        "\n",
        "def remove_nonenglish(tokens):\n",
        "    english_tokens = [word for word in tokens if word in englishwords]\n",
        "    return english_tokens\n",
        "\n",
        "def lemmatize(tokens):\n",
        "    lemmatized_tokens = [nltk.WordNetLemmatizer().lemmatize(word) for word in tokens]\n",
        "    return lemmatized_tokens\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \n",
        "    text_nopunctuation = remove_punctuation(text)\n",
        "    \n",
        "    text_tokens = tokenize(text_nopunctuation.lower())\n",
        "    \n",
        "    tokens_nostopwords = remove_stopwords(text_tokens)\n",
        "    \n",
        "    tokens_aplha = keeponly_alpha(tokens_nostopwords)\n",
        "    \n",
        "    #tokens_english = remove_nonenglish(tokens_aplha)\n",
        "    \n",
        "    tokens_lemmatized = lemmatize(tokens_aplha)\n",
        "    \n",
        "    return tokens_lemmatized\n",
        "\n",
        "def generate_label(rating):\n",
        "    if(rating >= 3):\n",
        "        return 1 #positive\n",
        "    else:\n",
        "        return 0 #negative\n",
        "    \n",
        "def word_frequency(text):\n",
        "    for word in text:\n",
        "        if word not in wordfrequency:\n",
        "            wordfrequency[word] = 1\n",
        "        else:\n",
        "            wordfrequency[word] += 1\n",
        "            \n",
        "def create_less_frequent_list():\n",
        "    for word in wordfrequency:\n",
        "        if(wordfrequency.get(word) < 20):\n",
        "            lessfrequentwords.append(word)     \n",
        "            \n",
        "def remove_lessfrequentwords(tokens):\n",
        "    frequent_tokens = \" \".join([word for word in tokens if word not in lessfrequentwords])\n",
        "    return frequent_tokens\n",
        "\n",
        "def create_unimportantwords(text):\n",
        "    text_nopunctuation = remove_punctuation(text)\n",
        "    text_tokens = tokenize(text_nopunctuation.lower())\n",
        "    for word in text_tokens:\n",
        "        if(word not in unimportant_words):\n",
        "            unimportant_words.append(word)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1-6t9RS9f5Vt",
        "colab": {}
      },
      "source": [
        "#%% data preprocessing\n",
        "\n",
        "#remove nan\n",
        "data.dropna(axis='rows', inplace=True)\n",
        "\n",
        "#generate label column\n",
        "data['class'] = data['star_rating'].apply(lambda x: generate_label(x))\n",
        "\n",
        "#preprocess pipeline\n",
        "data['review_body_cleaned'] = data['review_body'].apply(lambda x: preprocess_text(x))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMOQAt20gD9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% feature reduction\n",
        "wordfrequency = {}\n",
        "lessfrequentwords = []\n",
        "data['review_body_cleaned'].apply(lambda x: word_frequency(x))\n",
        "create_less_frequent_list()\n",
        "\n",
        "data['review_body_cleaned'] = data['review_body_cleaned'].apply(lambda x: remove_lessfrequentwords(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7i6yK9-tff21",
        "outputId": "33976942-ae3e-4fe7-c183-55890c891ef6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#load preprocessed data\n",
        "\n",
        "#import data\n",
        "data = pd.read_csv(\"/content/data_WithOutProductTitleWords_WithoutLessFrequentWords.tsv\", sep='\\t')\n",
        "\n",
        "data = data[['product_title', 'product_category', 'review_headline', 'review_body', 'star_rating', 'review_body_cleaned', 'class']]\n",
        "\n",
        "data.dropna(axis=0, inplace=True)\n",
        "\n",
        "data['review_body_cleaned'].isnull().sum()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90kTXRes4eO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#split data set before vectorization\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(data, test_size = 0.2, stratify = data['class'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_Si79LJhwM8Q",
        "colab": {}
      },
      "source": [
        "#vectorize\n",
        "#count vectorization\n",
        "\n",
        "count_vectorizer = CountVectorizer(lowercase=False)\n",
        "count_vectorizer_fit = count_vectorizer.fit(train['review_body_cleaned'])\n",
        "\n",
        "X_train = count_vectorizer_fit.transform(train['review_body_cleaned'])\n",
        "X_test = count_vectorizer_fit.transform(test['review_body_cleaned'])\n",
        "\n",
        "X_train = pd.DataFrame(X_train.toarray())\n",
        "X_train.columns = count_vectorizer_fit.get_feature_names()\n",
        "\n",
        "X_test = pd.DataFrame(X_test.toarray())\n",
        "X_test.columns = count_vectorizer_fit.get_feature_names()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DiCsTR7SXQcz",
        "outputId": "89988619-2f1a-4c05-e81c-6b40c232a435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(81496, 3700)\n",
            "(20374, 3700)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kPH7vuOfv-qh",
        "colab": {}
      },
      "source": [
        "#undersampling \n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "                                     \n",
        "randomundsamplr = RandomUnderSampler(random_state=0, replacement=True)\n",
        "sampled_X_df, sampled_class = randomundsamplr.fit_sample(X_df, data['class'])\n",
        "\n",
        "print(sampled_X_df.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mYycHdb5zl7D",
        "outputId": "b3c91171-5201-4a9f-b721-b5187d42384f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#using Naive Bayes classfier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "classifier_NB = GaussianNB()\n",
        "#scores = cross_val_score(classifier, X_train, train['class'], cv=5, scoring='accuracy')\n",
        "#print(scores)\n",
        "\n",
        "predicted = classifier_NB.fit(X_train, train['class']).predict(X_test)\n",
        "accuracy_score(test['class'], predicted)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4335918327279866"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W99ErZadQ61d",
        "colab_type": "code",
        "outputId": "9e4da81f-af54-4809-ffc9-67b15db9f3c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "X_test.columns[X_test.loc[0].values==1]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['customer', 'great', 'outstanding', 'perfect', 'perfection', 'problem',\n",
              "       'protect', 'smooth', 'work'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJaifvpxOJNV",
        "colab_type": "code",
        "outputId": "3fedef61-5f9c-4f8d-aa45-7272d19d4a18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test = count_vectorizer_fit.transform(['like it'])\n",
        "test = pd.DataFrame(test.toarray())\n",
        "test.columns = count_vectorizer_fit.get_feature_names()\n",
        "\n",
        "\n",
        "predicted = classifier_NB.predict(test)\n",
        "\n",
        "print(predicted)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_8uBw82XE9Lo",
        "outputId": "68359cc7-b74a-47b0-a1c6-b972541c2841",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#using Random Forest Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "classifier_RF = RandomForestClassifier(n_jobs=-1)\n",
        "#scores_RF = cross_val_score(classifier_RF, X_df, data['class'], cv=5, scoring='accuracy')\n",
        "\n",
        "predicted = classifier_RF.fit(X_train, train['class']).predict(X_test)\n",
        "accuracy_score(test['class'], predicted)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8501030725434378"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avCaHeZA_QOe",
        "colab_type": "code",
        "outputId": "e4cef0a8-4328-45b8-b789-6d6356bf4cb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test = count_vectorizer_fit.transform(['product is bad'])\n",
        "test = pd.DataFrame(test.toarray())\n",
        "test.columns = count_vectorizer_fit.get_feature_names()\n",
        "\n",
        "predicted = classifier_RF.predict(test)\n",
        "print(predicted)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i8px-kwYUo8Z",
        "colab": {}
      },
      "source": [
        " #using SVM Classifier\n",
        "from sklearn import svm\n",
        "classifier_SVM = svm.SVC(kernel='linear', gamma='scale')\n",
        "\n",
        "#scores_SVM = cross_val_score(classifier_SVM, X_df, data['class'], cv=5, scoring='accuracy')\n",
        "#print(scores_SVM) \n",
        "\n",
        "predicted = classifier_SVM.fit(X_train, train['class']).predict(X_test)\n",
        "accuracy_score(test['class'], predicted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y6shMXiELLLl",
        "colab": {}
      },
      "source": [
        "#ngram vectorization\n",
        "count_vectorizer_ngram = CountVectorizer(ngram_range=(1,2), lowercase=False, max_features=13000)\n",
        "count_vectorizer_ngram_fit = count_vectorizer_ngram.fit(train['review_body_cleaned'])\n",
        "\n",
        "X_train = count_vectorizer_ngram_fit.transform(train['review_body_cleaned'])\n",
        "X_test = count_vectorizer_ngram_fit.transform(test['review_body_cleaned'])\n",
        "\n",
        "X_train = pd.DataFrame(X_train.toarray())\n",
        "X_train.columns = count_vectorizer_ngram_fit.get_feature_names()\n",
        "\n",
        "X_test = pd.DataFrame(X_test.toarray())\n",
        "X_test.columns = count_vectorizer_ngram_fit.get_feature_names()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e86qxE5OgD9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P9YbwxZHQs_1",
        "colab": {}
      },
      "source": [
        "#using Naive Bayes classfier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "classifier = GaussianNB()\n",
        "#scores = cross_val_score(classifier, X_train, train['class'], cv=5, scoring='accuracy')\n",
        "#print(scores)\n",
        "\n",
        "predicted = classifier.fit(X_train, train['class']).predict(X_test)\n",
        "accuracy_score(test['class'], predicted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "guUp2NCpRiqG",
        "colab": {}
      },
      "source": [
        "#using Random Forest Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "classifier_RF = RandomForestClassifier(n_jobs=-1)\n",
        "scores_RF = cross_val_score(classifier_RF, X_df, data['class'], cv=5, scoring='accuracy')\n",
        "\n",
        "print(scores_RF)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Lg1UBjOLNE5P",
        "colab": {}
      },
      "source": [
        "#using SVM Classifier\n",
        "from sklearn import svm\n",
        "classifier_SVM = svm.SVC()\n",
        "\n",
        "scores_SVM = cross_val_score(classifier_SVM, X_df, data['class'], cv=5, scoring='accuracy')\n",
        "\n",
        "print(scores_SVM)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}